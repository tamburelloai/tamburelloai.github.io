<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>backend API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>backend</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">import json
import asent
import spacy
from sklearn.manifold import TSNE
from spacy import displacy
from managers.news_api_manager import NewsAPIManager
import pandas as pd
import numpy as np
from transformers import pipeline
from sklearn.decomposition import PCA
from utils import left_wing_sources, right_wing_sources

def load_secrets():
    res = {}
    with open(&#39;files/secrets.txt&#39;, &#39;r&#39;) as f:
        for line in f.readlines():
            line = line.split(&#39;=&#39;)
            assert len(line) == 2
            source, apikey = line
            apikey = apikey.replace(&#39;\n&#39;, &#39;&#39;)
            res[source] = apikey
    return res

secrets = load_secrets()

class Backend:
    def __init__(self):
        &#34;&#34;&#34;
        Initializes the Backend class.
            - Loads secrets for API access.
            - Initializes news API manager with API key.
            - Loads Spacy NLP model and sentiment model.
            - Sets up left-wing and right-wing news sources.
        &#34;&#34;&#34;
        secrets = load_secrets()
        self.newsapi = NewsAPIManager(secrets[&#39;newsapi&#39;])
        self.nlp_model = spacy.load(&#34;en_core_web_md&#34;)
        self.sentiment_model = spacy.blank(&#39;en&#39;)
        self.sentiment_model.add_pipe(&#39;sentencizer&#39;)
        self.sentiment_model.add_pipe(&#39;asent_en_v1&#39;)
        #
        self.leftwing_sources = left_wing_sources
        self.rightwing_sources = right_wing_sources
        self.query = None
        self.articles = None
        self.total_results = 0
        self.num_leftwing_results = 0
        self.num_rightwing_results = 0
        self.parsed_text = None


    def run_query(self, query):
        &#34;&#34;&#34;
        Executes a news query and processes the results.
        -  If in testing mode, loads sample data.
        - Otherwise, fetches news articles from left-wing and right-wing sources.
        - Processes the query results for further analysis.
        :param query: A string representing the news query.
       &#34;&#34;&#34;
        testing = False

        if testing:
            print(&#39;testing true&#39;)
            with open(&#39;samples/newsapi_leftwing_sample.json&#39;, &#39;r&#39;) as f:
                self.leftwing_response = json.load(f)
                self.num_leftwing_results = self.leftwing_response[&#39;totalResults&#39;]
                self.leftwing_response = pd.DataFrame(self.leftwing_response)[&#39;articles&#39;].tolist()
            with open(&#39;samples/newsapi_rightwing_sample.json&#39;, &#39;r&#39;) as f:
                self.rightwing_response = json.load(f)
                self.num_rightwing_results = self.rightwing_response[&#39;totalResults&#39;]
                self.rightwing_response = pd.DataFrame(self.rightwing_response)[&#39;articles&#39;].tolist()
        else:
            self.leftwing_response = self.newsapi(query=query, sources=&#39;, &#39;.join(self.leftwing_sources))
            self.num_leftwing_results = self.leftwing_response[&#39;totalResults&#39;]
            self.leftwing_response = pd.DataFrame(self.leftwing_response)[&#39;articles&#39;].tolist()

            self.rightwing_response = self.newsapi(query=query, sources=&#39;, &#39;.join(self.rightwing_sources))
            self.num_rightwing_results = self.rightwing_response[&#39;totalResults&#39;]
            self.rightwing_response = pd.DataFrame(self.rightwing_response)[&#39;articles&#39;].tolist()
        self.process_query_results()

    def sort_titles_by_similarity(self, leftwing_titles, rightwing_titles):
        &#34;&#34;&#34;
        Sorts news article titles by their similarity.
        - Compares titles from left-wing and right-wing sources.
        - Pairs titles based on similarity scores.

        :param leftwing_titles: A list of strings containing titles from left-wing sources.
        :param rightwing_titles: A list of strings containing titles from right-wing sources.
        :return: Two lists of sorted titles from left-wing and right-wing sources.
        &#34;&#34;&#34;
        # Create document objects for each sentence
        left_docs = [self.nlp_model(sentence) for sentence in leftwing_titles]
        right_docs = [self.nlp_model(sentence) for sentence in rightwing_titles]

        # Initialize a list to store the pairs
        paired_titles = []

        # Iterate over each document in left_docs with its index
        for left_index, left_doc in enumerate(left_docs):
            # Initialize a variable to store the highest similarity score and corresponding index
            max_similarity = 0
            most_similar_index = -1

            # Iterate over each document in right_docs with its index
            for right_index, right_doc in enumerate(right_docs):
                # Calculate similarity
                similarity = left_doc.similarity(right_doc)

                # Check if this is the highest similarity so far
                if similarity &gt; max_similarity:
                    max_similarity = similarity
                    most_similar_index = right_index

            # Add the most similar pair to the list
            if most_similar_index != -1:
                paired_titles.append((leftwing_titles[left_index], rightwing_titles[most_similar_index]))

        left_output = [x[0] for x in paired_titles]
        right_output = [x[1] for x in paired_titles]
        return left_output, right_output

    def process_query_results(self):
        &#34;&#34;&#34;
        Processes the results of a news query.
        - Extracts titles from left-wing and right-wing responses.
        - Sorts titles by similarity and analyzes sentiment.
        &#34;&#34;&#34;
        assert self.leftwing_response and self.rightwing_response
        # extract titles from response
        leftwing_titles = pd.DataFrame(self.leftwing_response)[&#39;title&#39;].tolist()
        rightwing_titles = pd.DataFrame(self.rightwing_response)[&#39;title&#39;].tolist()
        #sort by similarity to pair documents regardless of polarity/sentiment
        self.leftwing_titles, self.rightwing_titles = self.sort_titles_by_similarity(leftwing_titles, rightwing_titles)
        # get sentiment of titles for coloring
        self.leftwing_dataframe = self.build_response_dataset(self.leftwing_titles)
        self.rightwing_dataframe = self.build_response_dataset(self.rightwing_titles)

    def map_sentiment_to_color(self, value):
        &#34;&#34;&#34;
        Maps sentiment values to corresponding colors.
        - Currently a placeholder for future implementation.

        :param value: A sentiment value to map.
        :return: A color representation of the sentiment.
        &#34;&#34;&#34;
        return value

    def build_response_dataset(self, titles):
        &#34;&#34;&#34;
        Builds a dataset from news titles with associated sentiment scores.
        - Computes sentiment for each title.
        - Filters out titles with neutral sentiment.

        :param titles: A list of news titles.
        :return: A pandas DataFrame with titles and their sentiment scores.
        &#34;&#34;&#34;
        sentiment = []
        for t in titles:
            sentiment.append(self.map_sentiment_to_color(self.get_sentiment(t)[&#39;compound&#39;]))

        final_titles, final_sentiments = [], []
        for i in range(len(sentiment)):
            if sentiment[i] != 0:
                final_titles.append(titles[i])
                final_sentiments.append(sentiment[i])
        return pd.DataFrame({&#39;title&#39;: final_titles, &#39;sentiment&#39;: final_sentiments})


    def get_entity_breakdown_html(self, query_string):
        &#34;&#34;&#34;
        Generates HTML visualization for named entities in a text.
        - Uses Spacy&#39;s displacy for rendering entities.

        :param query_string: A string to analyze for named entities.
        :return: A string containing HTML for the entity visualization.
        &#34;&#34;&#34;
        doc = self.nlp_model(query_string)
        colors = {&#34;ORG&#34;: &#34;linear-gradient(90deg, #aa9cfc, #fc9ce7)&#34;}
        options = {&#34;colors&#34;: colors}
        ent_html = displacy.render(doc, style=&#34;ent&#34;, options=options, jupyter=False)
        return ent_html

    def get_sentiment(self, data_i):
        &#34;&#34;&#34;
        Analyzes sentiment of a given text.
        - Uses the sentiment model to calculate sentiment scores.

        :param data_i: A string for which sentiment analysis is performed.
        :return: A dictionary with sentiment scores.
        &#34;&#34;&#34;
        return dict(self.sentiment_model(data_i)._.polarity)

    def get_aggregate_sentiment(self, data):
        &#34;&#34;&#34;
        Computes aggregate sentiment for a collection of texts.
        - Determines overall sentiment and the most significant positive and negative cases.
        - Generates a visualization of the most significant sentiment.

        :param data: A list of strings to analyze.
        :return: A dictionary containing aggregate sentiment information.
        &#34;&#34;&#34;
        data = data.tolist()
        negsum, possum = 0, 0
        most_negative = (None, float(&#39;-inf&#39;))
        most_positive = (None, float(&#39;-inf&#39;))
        for d in data:
            doc = self.get_sentiment(d)
            negsum += doc[&#39;negative&#39;]
            possum += doc[&#39;positive&#39;]
            if abs(doc[&#39;negative&#39;]) &gt; most_negative[1]:
                most_negative = (doc, abs(doc[&#39;negative&#39;]))
            if abs(doc[&#39;positive&#39;]) &gt; most_positive[1]:
                most_positive = (doc, abs(doc[&#39;positive&#39;]))
        res = {&#39;positive&#39;: possum, &#39;negative&#39;: negsum}
        delta = max(possum, negsum) - min(possum, negsum)
        # Find the key with the maximum value
        max_key = max(res, key=res.get)
        max_value = res[max_key]

        if max_key == &#39;negative&#39;:
            svg_image = asent.visualize(most_negative[0], style=&#39;prediction&#39;)
        else:
            svg_image = asent.visualize(most_positive[0], style=&#39;prediction&#39;)
        return {&#39;sentiment&#39;: max_key, &#39;sentiment_score&#39;: max_value, &#39;delta&#39;: delta, &#39;svg_image&#39;: svg_image}

    def build_embedding_matrix(self):
        &#34;&#34;&#34;
        Builds an embedding matrix for words in news titles.
        - Uses NLP model to get word embeddings.
        - Applies t-SNE for dimensionality reduction.
        - Categorizes words based on their occurrence in different news sources.
        &#34;&#34;&#34;
        # get most commonly used words in english language to serve as baseline cluster
        most_common_100 = set(pd.read_csv(&#39;mostcommonwords.csv&#39;).iloc[:, 0].tolist())
        # Extract words from titles
        leftwing_words = [word for sentence in self.leftwing_titles for word in sentence.split() if word.isalpha()]
        rightwing_words = [word for sentence in self.rightwing_titles for word in sentence.split() if word.isalpha()]
        # Calculate intersection and unique words (found in articles returned by query)
        intersection = set(leftwing_words) &amp; set(rightwing_words)
        strictly_leftwing_words = set(leftwing_words) - intersection
        strictly_rightwing_words = set(rightwing_words) - intersection
        # Combine all unique words
        all_words = list(intersection | strictly_leftwing_words | strictly_rightwing_words | most_common_100)
        # Get embeddings
        docs = [self.nlp_model(word) for word in all_words]
        # Dimensionality reduction with t-SNE
        tsne = TSNE(n_components=3, random_state=0)
        embeddings = np.array([word.vector for word in docs])
        reduced_embeddings = tsne.fit_transform(embeddings)
        reduced_embeddings = (reduced_embeddings - reduced_embeddings.min(axis=0)) / (reduced_embeddings.max(axis=0) - reduced_embeddings.min(axis=0))
        x = reduced_embeddings[:, 0].reshape(-1)
        y = reduced_embeddings[:, 1].reshape(-1)
        z = reduced_embeddings[:, 2].reshape(-1)
        labels = [&#39;intersection&#39; if (word in intersection or word in most_common_100) else
                  &#39;left-wing&#39; if (word in strictly_leftwing_words) else
                  &#39;right-wing&#39; for word in all_words]

        self.embedding_projection_data =  pd.DataFrame({&#39;word&#39;: all_words,
                             &#39;x&#39;: x,
                             &#39;y&#39;: y,
                             &#39;z&#39;: z,
                             &#39;partisan&#39;: labels,
                             })</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="backend.load_secrets"><code class="name flex">
<span>def <span class="ident">load_secrets</span></span>(<span>)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def load_secrets():
    res = {}
    with open(&#39;files/secrets.txt&#39;, &#39;r&#39;) as f:
        for line in f.readlines():
            line = line.split(&#39;=&#39;)
            assert len(line) == 2
            source, apikey = line
            apikey = apikey.replace(&#39;\n&#39;, &#39;&#39;)
            res[source] = apikey
    return res</code></pre>
</details>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="backend.Backend"><code class="flex name class">
<span>class <span class="ident">Backend</span></span>
</code></dt>
<dd>
<div class="desc"><p>Initializes the Backend class.
- Loads secrets for API access.
- Initializes news API manager with API key.
- Loads Spacy NLP model and sentiment model.
- Sets up left-wing and right-wing news sources.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Backend:
    def __init__(self):
        &#34;&#34;&#34;
        Initializes the Backend class.
            - Loads secrets for API access.
            - Initializes news API manager with API key.
            - Loads Spacy NLP model and sentiment model.
            - Sets up left-wing and right-wing news sources.
        &#34;&#34;&#34;
        secrets = load_secrets()
        self.newsapi = NewsAPIManager(secrets[&#39;newsapi&#39;])
        self.nlp_model = spacy.load(&#34;en_core_web_md&#34;)
        self.sentiment_model = spacy.blank(&#39;en&#39;)
        self.sentiment_model.add_pipe(&#39;sentencizer&#39;)
        self.sentiment_model.add_pipe(&#39;asent_en_v1&#39;)
        #
        self.leftwing_sources = left_wing_sources
        self.rightwing_sources = right_wing_sources
        self.query = None
        self.articles = None
        self.total_results = 0
        self.num_leftwing_results = 0
        self.num_rightwing_results = 0
        self.parsed_text = None


    def run_query(self, query):
        &#34;&#34;&#34;
        Executes a news query and processes the results.
        -  If in testing mode, loads sample data.
        - Otherwise, fetches news articles from left-wing and right-wing sources.
        - Processes the query results for further analysis.
        :param query: A string representing the news query.
       &#34;&#34;&#34;
        testing = False

        if testing:
            print(&#39;testing true&#39;)
            with open(&#39;samples/newsapi_leftwing_sample.json&#39;, &#39;r&#39;) as f:
                self.leftwing_response = json.load(f)
                self.num_leftwing_results = self.leftwing_response[&#39;totalResults&#39;]
                self.leftwing_response = pd.DataFrame(self.leftwing_response)[&#39;articles&#39;].tolist()
            with open(&#39;samples/newsapi_rightwing_sample.json&#39;, &#39;r&#39;) as f:
                self.rightwing_response = json.load(f)
                self.num_rightwing_results = self.rightwing_response[&#39;totalResults&#39;]
                self.rightwing_response = pd.DataFrame(self.rightwing_response)[&#39;articles&#39;].tolist()
        else:
            self.leftwing_response = self.newsapi(query=query, sources=&#39;, &#39;.join(self.leftwing_sources))
            self.num_leftwing_results = self.leftwing_response[&#39;totalResults&#39;]
            self.leftwing_response = pd.DataFrame(self.leftwing_response)[&#39;articles&#39;].tolist()

            self.rightwing_response = self.newsapi(query=query, sources=&#39;, &#39;.join(self.rightwing_sources))
            self.num_rightwing_results = self.rightwing_response[&#39;totalResults&#39;]
            self.rightwing_response = pd.DataFrame(self.rightwing_response)[&#39;articles&#39;].tolist()
        self.process_query_results()

    def sort_titles_by_similarity(self, leftwing_titles, rightwing_titles):
        &#34;&#34;&#34;
        Sorts news article titles by their similarity.
        - Compares titles from left-wing and right-wing sources.
        - Pairs titles based on similarity scores.

        :param leftwing_titles: A list of strings containing titles from left-wing sources.
        :param rightwing_titles: A list of strings containing titles from right-wing sources.
        :return: Two lists of sorted titles from left-wing and right-wing sources.
        &#34;&#34;&#34;
        # Create document objects for each sentence
        left_docs = [self.nlp_model(sentence) for sentence in leftwing_titles]
        right_docs = [self.nlp_model(sentence) for sentence in rightwing_titles]

        # Initialize a list to store the pairs
        paired_titles = []

        # Iterate over each document in left_docs with its index
        for left_index, left_doc in enumerate(left_docs):
            # Initialize a variable to store the highest similarity score and corresponding index
            max_similarity = 0
            most_similar_index = -1

            # Iterate over each document in right_docs with its index
            for right_index, right_doc in enumerate(right_docs):
                # Calculate similarity
                similarity = left_doc.similarity(right_doc)

                # Check if this is the highest similarity so far
                if similarity &gt; max_similarity:
                    max_similarity = similarity
                    most_similar_index = right_index

            # Add the most similar pair to the list
            if most_similar_index != -1:
                paired_titles.append((leftwing_titles[left_index], rightwing_titles[most_similar_index]))

        left_output = [x[0] for x in paired_titles]
        right_output = [x[1] for x in paired_titles]
        return left_output, right_output

    def process_query_results(self):
        &#34;&#34;&#34;
        Processes the results of a news query.
        - Extracts titles from left-wing and right-wing responses.
        - Sorts titles by similarity and analyzes sentiment.
        &#34;&#34;&#34;
        assert self.leftwing_response and self.rightwing_response
        # extract titles from response
        leftwing_titles = pd.DataFrame(self.leftwing_response)[&#39;title&#39;].tolist()
        rightwing_titles = pd.DataFrame(self.rightwing_response)[&#39;title&#39;].tolist()
        #sort by similarity to pair documents regardless of polarity/sentiment
        self.leftwing_titles, self.rightwing_titles = self.sort_titles_by_similarity(leftwing_titles, rightwing_titles)
        # get sentiment of titles for coloring
        self.leftwing_dataframe = self.build_response_dataset(self.leftwing_titles)
        self.rightwing_dataframe = self.build_response_dataset(self.rightwing_titles)

    def map_sentiment_to_color(self, value):
        &#34;&#34;&#34;
        Maps sentiment values to corresponding colors.
        - Currently a placeholder for future implementation.

        :param value: A sentiment value to map.
        :return: A color representation of the sentiment.
        &#34;&#34;&#34;
        return value

    def build_response_dataset(self, titles):
        &#34;&#34;&#34;
        Builds a dataset from news titles with associated sentiment scores.
        - Computes sentiment for each title.
        - Filters out titles with neutral sentiment.

        :param titles: A list of news titles.
        :return: A pandas DataFrame with titles and their sentiment scores.
        &#34;&#34;&#34;
        sentiment = []
        for t in titles:
            sentiment.append(self.map_sentiment_to_color(self.get_sentiment(t)[&#39;compound&#39;]))

        final_titles, final_sentiments = [], []
        for i in range(len(sentiment)):
            if sentiment[i] != 0:
                final_titles.append(titles[i])
                final_sentiments.append(sentiment[i])
        return pd.DataFrame({&#39;title&#39;: final_titles, &#39;sentiment&#39;: final_sentiments})


    def get_entity_breakdown_html(self, query_string):
        &#34;&#34;&#34;
        Generates HTML visualization for named entities in a text.
        - Uses Spacy&#39;s displacy for rendering entities.

        :param query_string: A string to analyze for named entities.
        :return: A string containing HTML for the entity visualization.
        &#34;&#34;&#34;
        doc = self.nlp_model(query_string)
        colors = {&#34;ORG&#34;: &#34;linear-gradient(90deg, #aa9cfc, #fc9ce7)&#34;}
        options = {&#34;colors&#34;: colors}
        ent_html = displacy.render(doc, style=&#34;ent&#34;, options=options, jupyter=False)
        return ent_html

    def get_sentiment(self, data_i):
        &#34;&#34;&#34;
        Analyzes sentiment of a given text.
        - Uses the sentiment model to calculate sentiment scores.

        :param data_i: A string for which sentiment analysis is performed.
        :return: A dictionary with sentiment scores.
        &#34;&#34;&#34;
        return dict(self.sentiment_model(data_i)._.polarity)

    def get_aggregate_sentiment(self, data):
        &#34;&#34;&#34;
        Computes aggregate sentiment for a collection of texts.
        - Determines overall sentiment and the most significant positive and negative cases.
        - Generates a visualization of the most significant sentiment.

        :param data: A list of strings to analyze.
        :return: A dictionary containing aggregate sentiment information.
        &#34;&#34;&#34;
        data = data.tolist()
        negsum, possum = 0, 0
        most_negative = (None, float(&#39;-inf&#39;))
        most_positive = (None, float(&#39;-inf&#39;))
        for d in data:
            doc = self.get_sentiment(d)
            negsum += doc[&#39;negative&#39;]
            possum += doc[&#39;positive&#39;]
            if abs(doc[&#39;negative&#39;]) &gt; most_negative[1]:
                most_negative = (doc, abs(doc[&#39;negative&#39;]))
            if abs(doc[&#39;positive&#39;]) &gt; most_positive[1]:
                most_positive = (doc, abs(doc[&#39;positive&#39;]))
        res = {&#39;positive&#39;: possum, &#39;negative&#39;: negsum}
        delta = max(possum, negsum) - min(possum, negsum)
        # Find the key with the maximum value
        max_key = max(res, key=res.get)
        max_value = res[max_key]

        if max_key == &#39;negative&#39;:
            svg_image = asent.visualize(most_negative[0], style=&#39;prediction&#39;)
        else:
            svg_image = asent.visualize(most_positive[0], style=&#39;prediction&#39;)
        return {&#39;sentiment&#39;: max_key, &#39;sentiment_score&#39;: max_value, &#39;delta&#39;: delta, &#39;svg_image&#39;: svg_image}

    def build_embedding_matrix(self):
        &#34;&#34;&#34;
        Builds an embedding matrix for words in news titles.
        - Uses NLP model to get word embeddings.
        - Applies t-SNE for dimensionality reduction.
        - Categorizes words based on their occurrence in different news sources.
        &#34;&#34;&#34;
        # get most commonly used words in english language to serve as baseline cluster
        most_common_100 = set(pd.read_csv(&#39;mostcommonwords.csv&#39;).iloc[:, 0].tolist())
        # Extract words from titles
        leftwing_words = [word for sentence in self.leftwing_titles for word in sentence.split() if word.isalpha()]
        rightwing_words = [word for sentence in self.rightwing_titles for word in sentence.split() if word.isalpha()]
        # Calculate intersection and unique words (found in articles returned by query)
        intersection = set(leftwing_words) &amp; set(rightwing_words)
        strictly_leftwing_words = set(leftwing_words) - intersection
        strictly_rightwing_words = set(rightwing_words) - intersection
        # Combine all unique words
        all_words = list(intersection | strictly_leftwing_words | strictly_rightwing_words | most_common_100)
        # Get embeddings
        docs = [self.nlp_model(word) for word in all_words]
        # Dimensionality reduction with t-SNE
        tsne = TSNE(n_components=3, random_state=0)
        embeddings = np.array([word.vector for word in docs])
        reduced_embeddings = tsne.fit_transform(embeddings)
        reduced_embeddings = (reduced_embeddings - reduced_embeddings.min(axis=0)) / (reduced_embeddings.max(axis=0) - reduced_embeddings.min(axis=0))
        x = reduced_embeddings[:, 0].reshape(-1)
        y = reduced_embeddings[:, 1].reshape(-1)
        z = reduced_embeddings[:, 2].reshape(-1)
        labels = [&#39;intersection&#39; if (word in intersection or word in most_common_100) else
                  &#39;left-wing&#39; if (word in strictly_leftwing_words) else
                  &#39;right-wing&#39; for word in all_words]

        self.embedding_projection_data =  pd.DataFrame({&#39;word&#39;: all_words,
                             &#39;x&#39;: x,
                             &#39;y&#39;: y,
                             &#39;z&#39;: z,
                             &#39;partisan&#39;: labels,
                             })</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="backend.Backend.build_embedding_matrix"><code class="name flex">
<span>def <span class="ident">build_embedding_matrix</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Builds an embedding matrix for words in news titles.
- Uses NLP model to get word embeddings.
- Applies t-SNE for dimensionality reduction.
- Categorizes words based on their occurrence in different news sources.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def build_embedding_matrix(self):
    &#34;&#34;&#34;
    Builds an embedding matrix for words in news titles.
    - Uses NLP model to get word embeddings.
    - Applies t-SNE for dimensionality reduction.
    - Categorizes words based on their occurrence in different news sources.
    &#34;&#34;&#34;
    # get most commonly used words in english language to serve as baseline cluster
    most_common_100 = set(pd.read_csv(&#39;mostcommonwords.csv&#39;).iloc[:, 0].tolist())
    # Extract words from titles
    leftwing_words = [word for sentence in self.leftwing_titles for word in sentence.split() if word.isalpha()]
    rightwing_words = [word for sentence in self.rightwing_titles for word in sentence.split() if word.isalpha()]
    # Calculate intersection and unique words (found in articles returned by query)
    intersection = set(leftwing_words) &amp; set(rightwing_words)
    strictly_leftwing_words = set(leftwing_words) - intersection
    strictly_rightwing_words = set(rightwing_words) - intersection
    # Combine all unique words
    all_words = list(intersection | strictly_leftwing_words | strictly_rightwing_words | most_common_100)
    # Get embeddings
    docs = [self.nlp_model(word) for word in all_words]
    # Dimensionality reduction with t-SNE
    tsne = TSNE(n_components=3, random_state=0)
    embeddings = np.array([word.vector for word in docs])
    reduced_embeddings = tsne.fit_transform(embeddings)
    reduced_embeddings = (reduced_embeddings - reduced_embeddings.min(axis=0)) / (reduced_embeddings.max(axis=0) - reduced_embeddings.min(axis=0))
    x = reduced_embeddings[:, 0].reshape(-1)
    y = reduced_embeddings[:, 1].reshape(-1)
    z = reduced_embeddings[:, 2].reshape(-1)
    labels = [&#39;intersection&#39; if (word in intersection or word in most_common_100) else
              &#39;left-wing&#39; if (word in strictly_leftwing_words) else
              &#39;right-wing&#39; for word in all_words]

    self.embedding_projection_data =  pd.DataFrame({&#39;word&#39;: all_words,
                         &#39;x&#39;: x,
                         &#39;y&#39;: y,
                         &#39;z&#39;: z,
                         &#39;partisan&#39;: labels,
                         })</code></pre>
</details>
</dd>
<dt id="backend.Backend.build_response_dataset"><code class="name flex">
<span>def <span class="ident">build_response_dataset</span></span>(<span>self, titles)</span>
</code></dt>
<dd>
<div class="desc"><p>Builds a dataset from news titles with associated sentiment scores.
- Computes sentiment for each title.
- Filters out titles with neutral sentiment.</p>
<p>:param titles: A list of news titles.
:return: A pandas DataFrame with titles and their sentiment scores.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def build_response_dataset(self, titles):
    &#34;&#34;&#34;
    Builds a dataset from news titles with associated sentiment scores.
    - Computes sentiment for each title.
    - Filters out titles with neutral sentiment.

    :param titles: A list of news titles.
    :return: A pandas DataFrame with titles and their sentiment scores.
    &#34;&#34;&#34;
    sentiment = []
    for t in titles:
        sentiment.append(self.map_sentiment_to_color(self.get_sentiment(t)[&#39;compound&#39;]))

    final_titles, final_sentiments = [], []
    for i in range(len(sentiment)):
        if sentiment[i] != 0:
            final_titles.append(titles[i])
            final_sentiments.append(sentiment[i])
    return pd.DataFrame({&#39;title&#39;: final_titles, &#39;sentiment&#39;: final_sentiments})</code></pre>
</details>
</dd>
<dt id="backend.Backend.get_aggregate_sentiment"><code class="name flex">
<span>def <span class="ident">get_aggregate_sentiment</span></span>(<span>self, data)</span>
</code></dt>
<dd>
<div class="desc"><p>Computes aggregate sentiment for a collection of texts.
- Determines overall sentiment and the most significant positive and negative cases.
- Generates a visualization of the most significant sentiment.</p>
<p>:param data: A list of strings to analyze.
:return: A dictionary containing aggregate sentiment information.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_aggregate_sentiment(self, data):
    &#34;&#34;&#34;
    Computes aggregate sentiment for a collection of texts.
    - Determines overall sentiment and the most significant positive and negative cases.
    - Generates a visualization of the most significant sentiment.

    :param data: A list of strings to analyze.
    :return: A dictionary containing aggregate sentiment information.
    &#34;&#34;&#34;
    data = data.tolist()
    negsum, possum = 0, 0
    most_negative = (None, float(&#39;-inf&#39;))
    most_positive = (None, float(&#39;-inf&#39;))
    for d in data:
        doc = self.get_sentiment(d)
        negsum += doc[&#39;negative&#39;]
        possum += doc[&#39;positive&#39;]
        if abs(doc[&#39;negative&#39;]) &gt; most_negative[1]:
            most_negative = (doc, abs(doc[&#39;negative&#39;]))
        if abs(doc[&#39;positive&#39;]) &gt; most_positive[1]:
            most_positive = (doc, abs(doc[&#39;positive&#39;]))
    res = {&#39;positive&#39;: possum, &#39;negative&#39;: negsum}
    delta = max(possum, negsum) - min(possum, negsum)
    # Find the key with the maximum value
    max_key = max(res, key=res.get)
    max_value = res[max_key]

    if max_key == &#39;negative&#39;:
        svg_image = asent.visualize(most_negative[0], style=&#39;prediction&#39;)
    else:
        svg_image = asent.visualize(most_positive[0], style=&#39;prediction&#39;)
    return {&#39;sentiment&#39;: max_key, &#39;sentiment_score&#39;: max_value, &#39;delta&#39;: delta, &#39;svg_image&#39;: svg_image}</code></pre>
</details>
</dd>
<dt id="backend.Backend.get_entity_breakdown_html"><code class="name flex">
<span>def <span class="ident">get_entity_breakdown_html</span></span>(<span>self, query_string)</span>
</code></dt>
<dd>
<div class="desc"><p>Generates HTML visualization for named entities in a text.
- Uses Spacy's displacy for rendering entities.</p>
<p>:param query_string: A string to analyze for named entities.
:return: A string containing HTML for the entity visualization.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_entity_breakdown_html(self, query_string):
    &#34;&#34;&#34;
    Generates HTML visualization for named entities in a text.
    - Uses Spacy&#39;s displacy for rendering entities.

    :param query_string: A string to analyze for named entities.
    :return: A string containing HTML for the entity visualization.
    &#34;&#34;&#34;
    doc = self.nlp_model(query_string)
    colors = {&#34;ORG&#34;: &#34;linear-gradient(90deg, #aa9cfc, #fc9ce7)&#34;}
    options = {&#34;colors&#34;: colors}
    ent_html = displacy.render(doc, style=&#34;ent&#34;, options=options, jupyter=False)
    return ent_html</code></pre>
</details>
</dd>
<dt id="backend.Backend.get_sentiment"><code class="name flex">
<span>def <span class="ident">get_sentiment</span></span>(<span>self, data_i)</span>
</code></dt>
<dd>
<div class="desc"><p>Analyzes sentiment of a given text.
- Uses the sentiment model to calculate sentiment scores.</p>
<p>:param data_i: A string for which sentiment analysis is performed.
:return: A dictionary with sentiment scores.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_sentiment(self, data_i):
    &#34;&#34;&#34;
    Analyzes sentiment of a given text.
    - Uses the sentiment model to calculate sentiment scores.

    :param data_i: A string for which sentiment analysis is performed.
    :return: A dictionary with sentiment scores.
    &#34;&#34;&#34;
    return dict(self.sentiment_model(data_i)._.polarity)</code></pre>
</details>
</dd>
<dt id="backend.Backend.map_sentiment_to_color"><code class="name flex">
<span>def <span class="ident">map_sentiment_to_color</span></span>(<span>self, value)</span>
</code></dt>
<dd>
<div class="desc"><p>Maps sentiment values to corresponding colors.
- Currently a placeholder for future implementation.</p>
<p>:param value: A sentiment value to map.
:return: A color representation of the sentiment.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def map_sentiment_to_color(self, value):
    &#34;&#34;&#34;
    Maps sentiment values to corresponding colors.
    - Currently a placeholder for future implementation.

    :param value: A sentiment value to map.
    :return: A color representation of the sentiment.
    &#34;&#34;&#34;
    return value</code></pre>
</details>
</dd>
<dt id="backend.Backend.process_query_results"><code class="name flex">
<span>def <span class="ident">process_query_results</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Processes the results of a news query.
- Extracts titles from left-wing and right-wing responses.
- Sorts titles by similarity and analyzes sentiment.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def process_query_results(self):
    &#34;&#34;&#34;
    Processes the results of a news query.
    - Extracts titles from left-wing and right-wing responses.
    - Sorts titles by similarity and analyzes sentiment.
    &#34;&#34;&#34;
    assert self.leftwing_response and self.rightwing_response
    # extract titles from response
    leftwing_titles = pd.DataFrame(self.leftwing_response)[&#39;title&#39;].tolist()
    rightwing_titles = pd.DataFrame(self.rightwing_response)[&#39;title&#39;].tolist()
    #sort by similarity to pair documents regardless of polarity/sentiment
    self.leftwing_titles, self.rightwing_titles = self.sort_titles_by_similarity(leftwing_titles, rightwing_titles)
    # get sentiment of titles for coloring
    self.leftwing_dataframe = self.build_response_dataset(self.leftwing_titles)
    self.rightwing_dataframe = self.build_response_dataset(self.rightwing_titles)</code></pre>
</details>
</dd>
<dt id="backend.Backend.run_query"><code class="name flex">
<span>def <span class="ident">run_query</span></span>(<span>self, query)</span>
</code></dt>
<dd>
<div class="desc"><p>Executes a news query and processes the results.
-
If in testing mode, loads sample data.
- Otherwise, fetches news articles from left-wing and right-wing sources.
- Processes the query results for further analysis.
:param query: A string representing the news query.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def run_query(self, query):
    &#34;&#34;&#34;
    Executes a news query and processes the results.
    -  If in testing mode, loads sample data.
    - Otherwise, fetches news articles from left-wing and right-wing sources.
    - Processes the query results for further analysis.
    :param query: A string representing the news query.
   &#34;&#34;&#34;
    testing = False

    if testing:
        print(&#39;testing true&#39;)
        with open(&#39;samples/newsapi_leftwing_sample.json&#39;, &#39;r&#39;) as f:
            self.leftwing_response = json.load(f)
            self.num_leftwing_results = self.leftwing_response[&#39;totalResults&#39;]
            self.leftwing_response = pd.DataFrame(self.leftwing_response)[&#39;articles&#39;].tolist()
        with open(&#39;samples/newsapi_rightwing_sample.json&#39;, &#39;r&#39;) as f:
            self.rightwing_response = json.load(f)
            self.num_rightwing_results = self.rightwing_response[&#39;totalResults&#39;]
            self.rightwing_response = pd.DataFrame(self.rightwing_response)[&#39;articles&#39;].tolist()
    else:
        self.leftwing_response = self.newsapi(query=query, sources=&#39;, &#39;.join(self.leftwing_sources))
        self.num_leftwing_results = self.leftwing_response[&#39;totalResults&#39;]
        self.leftwing_response = pd.DataFrame(self.leftwing_response)[&#39;articles&#39;].tolist()

        self.rightwing_response = self.newsapi(query=query, sources=&#39;, &#39;.join(self.rightwing_sources))
        self.num_rightwing_results = self.rightwing_response[&#39;totalResults&#39;]
        self.rightwing_response = pd.DataFrame(self.rightwing_response)[&#39;articles&#39;].tolist()
    self.process_query_results()</code></pre>
</details>
</dd>
<dt id="backend.Backend.sort_titles_by_similarity"><code class="name flex">
<span>def <span class="ident">sort_titles_by_similarity</span></span>(<span>self, leftwing_titles, rightwing_titles)</span>
</code></dt>
<dd>
<div class="desc"><p>Sorts news article titles by their similarity.
- Compares titles from left-wing and right-wing sources.
- Pairs titles based on similarity scores.</p>
<p>:param leftwing_titles: A list of strings containing titles from left-wing sources.
:param rightwing_titles: A list of strings containing titles from right-wing sources.
:return: Two lists of sorted titles from left-wing and right-wing sources.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def sort_titles_by_similarity(self, leftwing_titles, rightwing_titles):
    &#34;&#34;&#34;
    Sorts news article titles by their similarity.
    - Compares titles from left-wing and right-wing sources.
    - Pairs titles based on similarity scores.

    :param leftwing_titles: A list of strings containing titles from left-wing sources.
    :param rightwing_titles: A list of strings containing titles from right-wing sources.
    :return: Two lists of sorted titles from left-wing and right-wing sources.
    &#34;&#34;&#34;
    # Create document objects for each sentence
    left_docs = [self.nlp_model(sentence) for sentence in leftwing_titles]
    right_docs = [self.nlp_model(sentence) for sentence in rightwing_titles]

    # Initialize a list to store the pairs
    paired_titles = []

    # Iterate over each document in left_docs with its index
    for left_index, left_doc in enumerate(left_docs):
        # Initialize a variable to store the highest similarity score and corresponding index
        max_similarity = 0
        most_similar_index = -1

        # Iterate over each document in right_docs with its index
        for right_index, right_doc in enumerate(right_docs):
            # Calculate similarity
            similarity = left_doc.similarity(right_doc)

            # Check if this is the highest similarity so far
            if similarity &gt; max_similarity:
                max_similarity = similarity
                most_similar_index = right_index

        # Add the most similar pair to the list
        if most_similar_index != -1:
            paired_titles.append((leftwing_titles[left_index], rightwing_titles[most_similar_index]))

    left_output = [x[0] for x in paired_titles]
    right_output = [x[1] for x in paired_titles]
    return left_output, right_output</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="backend.load_secrets" href="#backend.load_secrets">load_secrets</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="backend.Backend" href="#backend.Backend">Backend</a></code></h4>
<ul class="">
<li><code><a title="backend.Backend.build_embedding_matrix" href="#backend.Backend.build_embedding_matrix">build_embedding_matrix</a></code></li>
<li><code><a title="backend.Backend.build_response_dataset" href="#backend.Backend.build_response_dataset">build_response_dataset</a></code></li>
<li><code><a title="backend.Backend.get_aggregate_sentiment" href="#backend.Backend.get_aggregate_sentiment">get_aggregate_sentiment</a></code></li>
<li><code><a title="backend.Backend.get_entity_breakdown_html" href="#backend.Backend.get_entity_breakdown_html">get_entity_breakdown_html</a></code></li>
<li><code><a title="backend.Backend.get_sentiment" href="#backend.Backend.get_sentiment">get_sentiment</a></code></li>
<li><code><a title="backend.Backend.map_sentiment_to_color" href="#backend.Backend.map_sentiment_to_color">map_sentiment_to_color</a></code></li>
<li><code><a title="backend.Backend.process_query_results" href="#backend.Backend.process_query_results">process_query_results</a></code></li>
<li><code><a title="backend.Backend.run_query" href="#backend.Backend.run_query">run_query</a></code></li>
<li><code><a title="backend.Backend.sort_titles_by_similarity" href="#backend.Backend.sort_titles_by_similarity">sort_titles_by_similarity</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>